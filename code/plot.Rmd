---
title: "plot"
output: html_document
---

```{r }
fancy <- read.csv("../data/fancyhouse.csv")
house <- read.csv("../data/housingprice.csv")
train <- read.csv("../data/train.data.csv")
test  <- read.csv("../data/test.data.csv")

library(kableExtra)
library(dplyr)
library(ggplot2)
options(digits=6)

```


### (a)
```{r }
# R2 indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.

train.house <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot, data = train)
# summary(train.house)$r.squared # 0.5101139

train.X <- as.matrix(cbind(rep(1, nrow(train)), apply(train[,5:8],2,scale)))
train.y <- as.matrix(train[,4])
fit <- lm(train.y ~ train.X[,-1])

test.X <- as.matrix(cbind(rep(1, nrow(test)), apply(test[,5:8],2,scale)))
test.y <- as.matrix(test[,4])

pred.y <- test.X %*% matrix(fit$coefficients, ncol = 1, nrow = ncol(test.X))
   
# function to calculate R-square
rsquare <- function(obs, fitted) {
  r2 = 1 - (sum((obs-fitted)^2)) / (sum((obs-mean(obs))^2))
  return(r2)
}
 
train.r2 <- rsquare(train$price, fit$fitted.values) # 0.5101139
test.r2 <- rsquare(test.y, pred.y) # 0.5049332

out <- cbind(train.r2, test.r2) %>% as.data.frame()
colnames(out) <- c("Training Data", "Test Data")
rownames(out) <- "R-square Value"

knitr::kable(out, align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

```{r }

### (b)
fit.house <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot, data = house)
pred <- predict(fit.house, fancy) # 15,390,168

out <- cbind("15,390,168", "127,000,000") %>% as.data.frame()
colnames(out) <- c("Predicted Value", "Real Value")
rownames(out) <- "Bill Gate's House"

knitr::kable(out, align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

```{r }

### (c)
train.house.2 <- lm(price ~ sqft_living+sqft_lot+bedrooms*bathrooms, data = train)
summary(train.house.2)

fit.2 <- lm(train.y ~ train.X[,2]*train.X[,3]+train.X[,4]+train.X[,5])
test.X.2 <- as.matrix(cbind(test.X, test.X[,2]*test.X[,3]))

pred.y.2 <- test.X.2 %*% matrix(fit.2$coefficients, ncol = 1, nrow = ncol(test.X.2))

train.r2.2 <- rsquare(train$price, fit.2$fitted.values) # 0.5173533
test.r2.2 <- rsquare(test.y, pred.y.2) # 0.5105277

out <- cbind(train.r2.2, test.r2.2) %>% as.data.frame()
colnames(out) <- c("Training Data", "Test Data")
rownames(out) <- "R-square Value"

knitr::kable(out, align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```




### (d)
```{r }
#check global variable in function 
checkStrict <- function(f, silent=FALSE) {
  vars <- codetools::findGlobals(f)
  found <- !vapply(vars, exists, logical(1), envir=as.environment(2))
  if (!silent && any(found)) {
    warning("global variables used: ", paste(names(found)[found], collapse=', '))
    return(invisible(FALSE))
  }
  !any(found)
}

```

```{r}
##################### For linear model fit with no interaction term:
dat.lm <- train[,c(5:8,4)]
dat.lm.scale <- scale(dat.lm)
y <- dat.lm.scale[,ncol(dat.lm.scale)]
X <- cbind(1, dat.lm.scale[,-ncol(dat.lm.scale)])

theta_opt_lm <- summary(lm(y ~ X[,2]+X[,3]+X[,4]+X[,5]))$coefficients[,1]

dat.lm.test <- test[,c(5:8,4)]

##################### For linear model fit with interaction term:
dat.lm.int <- cbind(train[c(5:8)], train[5]*train[6], train[4])
colnames(dat.lm.int) <- c("bedrooms", "bathrooms", "sqft_living","sqft_lot",
                         "interaction", "price")

dat.lm.int.scale <- scale(dat.lm.int)
y <- dat.lm.int.scale[,ncol(dat.lm.int.scale)]
X <- cbind(1, dat.lm.int.scale[,-ncol(dat.lm.int.scale)])

dat.lm.int.test <- cbind(test[c(5:8)], test[5]*test[6], test[4])
colnames(dat.lm.int.test) <- c("bedrooms", "bathrooms", "sqft_living","sqft_lot",
                         "interaction", "price")
dat.lm.int.scale.test <- scale(dat.lm.int.test)
y.test <- dat.lm.int.scale.test[,ncol(dat.lm.int.scale.test)]
X.test <- cbind(1, dat.lm.int.scale.test[,-ncol(dat.lm.int.scale.test)])

theta_opt_lm.int <- summary(lm(y ~ X[,2]+X[,3]+X[,4]+X[,5]+X[,6]))$coefficients[,1]



##################### Create a Gradient Descent Function:
gda <- function(data, eps = 0.001, max.iter = 50, standardize = T, seed=123,
                optimal.theta) {
  
  set.seed(seed)
  #scaling data 
  data <- as.matrix(data)
  p <- ncol(data)
  n <- nrow(data)
  
  mean.x = apply(data[,-p], 2, mean)
  sd.x = apply(data[,-p], 2, sd)
  mean.y = mean(data[,p])
  sd.y = sd(data[,p])
  
  if(standardize) {data <- scale(data)} # scale data if required
  
  #predictor and response 
  X <- data[,-p]
  y <- data[,p]
  
  #starting values of theta
  theta <- matrix(runif(n = p-1), 
                  ncol = p-1, nrow=1)
  theta.new <- theta
  
  ###gradient descent###
  
  #tuning parameter
  eigen <- eigen(t(X) %*% X, only.values = TRUE)
  stepsize <- 2 / (eigen$values[1] + eigen$values[p-1])
  
  #iteration
  step <- 1
  while ( step <= max.iter & norm(optimal.theta - theta) > eps ) { 
    theta <- theta.new
    res <- (X %*% t(theta)) - y
    gradient <- t(res) %*% X
    theta.new <- theta - gradient * stepsize
    step <- step + 1
    # print(norm(optimal.theta - theta))
  }
  beta = sd.y * theta[2:p-1] / sd.x
  intercept = -sd.y*sum(theta[2:p-1] * mean.x/sd.x) + mean.y
  result = c("Intercept"=intercept, beta)
  return(list(result,theta))
}

GD.out <- gda(train[,c(5:8,4)], standardize = TRUE, optimal.theta = theta_opt_lm[2:5])

GD.out.int <- gda(dat.lm.int, 
                  standardize = TRUE, optimal.theta = theta_opt_lm.int[2:6],
                  max.iter = 500)

## R-square for training data
get.r2 <- function (train = train[,c(5:8,4)],
                    test = test[,c(5:8,4)], SGD_result) {
  p = ncol(train)
  ## R-square for training data
  X.prep <- scale(train)
  X <- X.prep[,-ncol(X.prep)]
  pred.y <- X %*% t(SGD_result)
  r2.train <- rsquare(X.prep[,p], pred.y)

  ## R-square for test data
  X.prep <- scale(test)
  X <- X.prep[,-ncol(X.prep)]
  pred.y <- X %*% t(SGD_result)
  r2.test <- rsquare(X.prep[,p], pred.y)

  return(c(r2.train,r2.test))
}

r2.lm <- get.r2(train[,c(5:8,4)], test[,c(5:8,4)], SGD_result = GD.out[[2]])
r2.lm.int <- get.r2(dat.lm.int, dat.lm.int.test, SGD_result =GD.out.int[[2]])


noint.result = cbind(c(train.r2, test.r2) ,r2.lm)
colnames(noint.result) <- c("lm fit", "Gradient Descent")
rownames(noint.result) <- c("Training data", "test data")


knitr::kable(noint.result, align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

int.result = cbind(c(train.r2, test.r2) ,r2.lm.int)
colnames(int.result) <- c("lm fit", "Gradient Descent")
rownames(int.result) <- c("Training data", "test data")


knitr::kable(int.result, align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)


```



```{r}
sgda <- function(data, data_test, max.iter = 30, stepsize = 5, diminish = TRUE, 
                 standardize = T, seed=123) {

  set.seed(seed)
  #scaling data
  data <- as.matrix(data)
  data_test <- as.matrix(data_test)
  p = ncol(data)
  n = nrow(data)
  
  mean.x = apply(data[,-p], 2, mean)
  sd.x = apply(data[,-p], 2, sd)
  mean.y = mean(data[,p])
  sd.y = sd(data[,p])
  
  if (standardize) { 
    data <- scale(data)
    data_test <- scale(data_test)
  } # scale data if required

  #predictor and response
  X <- data[,-p]
  y <- data[,p]
  
  #test data
  X.test <- data_test[, -p]
  y.test <- data_test[,p]
  
  #starting values of theta
  theta <- matrix(runif(n = p-1), ncol = p-1, nrow = 1)
  train.loss <- norm(y - X %*% t(theta), "2") 
  test.loss <- norm(y.test - X.test %*% t(theta), "2")

  #iteration
  j <- 0
  ref <- 1
  stochastic.list <- sample(1:n, n)

  while ( j <= max.iter ) {
    j <- j + 1

    if(diminish){
      eta <- stepsize / (j + 1)
    }else{
      eta <- stepsize 
    }
    
    X.new <- X[stochastic.list[j],] %>% as.matrix()

    res <- ( t(X.new) %*% t(theta) ) - y[stochastic.list[j]]
    gradient <- t(res) %*% t(X.new)
    theta <- theta - gradient * eta
    ref <- norm(gradient,"2")
    train.loss <- c(train.loss, norm(y - X %*% t(theta), "2")   )
    test.loss <- c(test.loss, norm(y.test - X.test%*%t(theta), "2")  )
    # print(eta)
    # print(norm(optimal - theta))
  }
  
  color = c("train.loss" = "black", "test.loss" = "red")
  gg = ggplot() + 
    geom_point(aes(x = 1:length(test.loss), y = test.loss, color = "test.loss"), size = 0.5) + 
    geom_line(aes(x = 1:length(test.loss), y = test.loss, color = "test.loss"), size = 0.5)  + 
    geom_point(aes(x = 1:length(train.loss), y = train.loss, color = "train.loss"), size = 0.5) + 
    geom_line(aes(x = 1:length(train.loss),y  = train.loss, color = "train.loss"), size = 0.5) +
    xlab("iteration") + 
    ylab("L2 Loss") + 
    labs(title = "Loss function ", color = "Legend") +
    theme(plot.title = element_text(hjust = 0.5, size = 20)) + 
    scale_color_manual(values = color)
  
  show(gg)
   print(paste0("Rsquare for training data is ", rsquare(y, X%*%t(theta))))
   print(paste0("Rsquare for test data is ", rsquare(y.test, X.test %*% t(theta))))
  
  beta = sd.y * theta[2:p-1] / sd.x
  intercept = -sd.y*sum(theta[2:p-1] * mean.x/sd.x) + mean.y
  result = c("Intercept"=intercept, beta)
  return(list("original beta"=result, "scaled beta"=theta, "R2 train"=rsquare(y, X%*%t(theta)), 
              "R2 test"= rsquare(y.test, X.test %*% t(theta)) ))
}


##### NO interaction 
#fix stepsize
SGD.lm.dimi <- sgda(dat.lm, dat.lm.test, standardize = T, stepsize = 2, diminish = TRUE, max.iter = 2000) # 0.505301, 0.500935

#diminish stepsize
SGD.lm <- sgda(dat.lm, dat.lm.test, standardize = T, stepsize = 0.02, diminish = FALSE, max.iter = 1600) # 0.503264, 0.498047


##### interaction 
# SGD.out <- sgda(train[,c(5:8,4)], standardize = T, stepsize = 1, max.iter = 1000)
#fix stepsize
SGD.lm.int.dimi <- sgda(dat.lm.int, dat.lm.int.test, standardize = T, stepsize = 2, diminish = TRUE, max.iter = 2000) #Rsquare 0.514392 / 0.50883

SGD.lm.int <- sgda(dat.lm.int, dat.lm.int.test, standardize = T, stepsize = 0.02, diminish = FALSE, max.iter = 520) #Rsquare 0.498575 / 0.494554

```


```{r}
## make a table for comparison - no interaction
noint.result <- rbind(cbind(summary(train.house)$coefficients[,1],
                     GD.out[[1]],SGD.lm.dimi[[1]],SGD.lm[[1]]), 
                     cbind(c(train.r2, test.r2), c(r2.lm[1],r2.lm[2]),
                           c(SGD.lm.dimi[[3]],SGD.lm.dimi[[4]]),
                           c( SGD.lm[[3]],SGD.lm[[4]] )))
noint.result[1:5,] <- as.character(round(noint.result[1:5,], 2))
noint.result[6:7,] <- as.character(round(as.numeric(noint.result[6:7,]), 5))

colnames(noint.result) <- c("linear Model", "Gradient Descent", 
                   "SGD with diminishing stepsize", 
                   "SGD with fixed stepsize")
rownames(noint.result)[6:7] <- c("R2 Train", "R2 Test")

knitr::kable(noint.result[6:7,], align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```



```{r}
## make a table for comparison - no interaction
int.result = rbind(cbind(summary(train.house.2)$coefficients[,1],
                     GD.out.int[[1]], SGD.lm.int.dimi[[1]], SGD.lm.int[[1]]), 
                     cbind(c(train.r2.2, test.r2.2), c(r2.lm.int[1],r2.lm.int[2]),
                           c(SGD.lm.int.dimi[[3]],SGD.lm.int.dimi[[4]]),
                           c( SGD.lm.int[[3]],SGD.lm.int[[4]] )))
int.result[1:6,] = as.character(round(int.result[1:6,], 2))
int.result[7:8,] = as.character(round(as.numeric(int.result[7:8,]), 5))

colnames(int.result) <- c("linear Model", "Gradient Descent", 
                   "SGD with diminishing stepsize", 
                   "SGD with fixed stepsize")
rownames(int.result)[7:8] <- c("R2 Train", "R2 Test")

knitr::kable(int.result[7:8, ], align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```



